{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1aOQlPCC2cK"
      },
      "source": [
        "# Early Warning system\n",
        "\n",
        "### FINTECH Final Project - Group 21\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feyloBcsSUo_"
      },
      "source": [
        "### Import libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUYNgvJ7JU8b",
        "outputId": "2145cb35-eb00-4b4a-f514-5214c5e72c43"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "# Libraries\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Needed for 3D plotting\n",
        "\n",
        "# Statistics\n",
        "from scipy import stats\n",
        "from scipy.stats import multivariate_normal, jarque_bera\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "\n",
        "# Dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "import umap.umap_ as umap\n",
        "\n",
        "# Anomaly detection\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# Classification and modeling\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve,\n",
        "    matthews_corrcoef,\n",
        "    classification_report,\n",
        "    make_scorer\n",
        ")\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Optimization\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "# Explainability\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvEZ9J98JswU"
      },
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "file_path = \"Dataset.xlsx\"\n",
        "data_df = pd.read_excel(file_path, sheet_name='Markets')\n",
        "metadata_df = pd.read_excel(file_path, sheet_name='Metadata')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JfwE3SiNRdB_",
        "outputId": "874af960-b2c7-488d-a45a-05d8ce0ba2eb"
      },
      "outputs": [],
      "source": [
        "# Extract date and anomaly label columns\n",
        "date_col = 'Date' if 'Date' in data_df.columns else data_df.columns[0]\n",
        "y_col = 'Y' if 'Y' in data_df.columns else None\n",
        "\n",
        "# Convert dates to datetime format and set as index\n",
        "data_df[date_col] = pd.to_datetime(data_df[date_col], dayfirst=True)  # Date format is dd/mm/yy\n",
        "data_df.set_index(date_col, inplace=True)\n",
        "\n",
        "# Extract features (all columns except Y if it exists)\n",
        "if y_col:\n",
        "    X_df = data_df.drop(y_col, axis=1)\n",
        "    y = data_df[y_col].values\n",
        "else:\n",
        "    X_df = data_df\n",
        "    y = None\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"Data shape: {X_df.shape}\")\n",
        "print(f\"Total number of records: {len(X_df)}\")\n",
        "print(f\"Time period: from {X_df.index.min().strftime('%m/%d/%Y')} to {X_df.index.max().strftime('%m/%d/%Y')}\")\n",
        "print(f\"Frequency: {pd.infer_freq(X_df.index) or 'Weekly'}\")\n",
        "print(f\"Number of variables: {X_df.shape[1]}\")\n",
        "if y_col:\n",
        "    print(f\"Number of anomalies: {np.sum(y == 1)} ({np.mean(y == 1)*100:.2f}%)\")\n",
        "\n",
        "# Create a more comprehensive metadata table\n",
        "comprehensive_metadata = []\n",
        "\n",
        "# Determine the correct column names for ticker and description\n",
        "ticker_col = 'ticker' if 'ticker' in metadata_df.columns else metadata_df.columns[0]\n",
        "desc_col = 'description' if 'description' in metadata_df.columns else metadata_df.columns[1] if len(metadata_df.columns) > 1 else ticker_col\n",
        "\n",
        "for ticker in X_df.columns:\n",
        "    # Get metadata for this ticker if available\n",
        "    meta_row = metadata_df[metadata_df[ticker_col] == ticker] if ticker in metadata_df[ticker_col].values else pd.DataFrame()\n",
        "\n",
        "    # Get description or use ticker if not found\n",
        "    description = meta_row[desc_col].values[0] if not meta_row.empty and desc_col in meta_row.columns else ticker\n",
        "\n",
        "    # Calculate statistics for this series\n",
        "    series = X_df[ticker]\n",
        "\n",
        "    comprehensive_metadata.append({\n",
        "        'Ticker': ticker,\n",
        "        'Description': description,\n",
        "        'Mean': series.mean(),\n",
        "        'Std.Dev': series.std(),\n",
        "        'Min': series.min(),\n",
        "        'Max': series.max(),\n",
        "        'Missing values': series.isna().sum(),\n",
        "        'Missing (%)': f\"{series.isna().mean()*100:.2f}%\"\n",
        "    })\n",
        "\n",
        "# Create enhanced metadata dataframe\n",
        "comprehensive_meta_df = pd.DataFrame(comprehensive_metadata)\n",
        "\n",
        "# Display the enhanced metadata\n",
        "print(\"\\nMetadata and statistics:\")\n",
        "display(comprehensive_meta_df)\n",
        "\n",
        "# Visualize a single stock behaviour\n",
        "if y_col and 'MXUS' in X_df.columns:\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "    # Plot MXUS line\n",
        "    ax.plot(X_df.index, X_df['MXUS'], color='darkred', linewidth=2.5, label='MSCI USA')\n",
        "\n",
        "    # Get the y-axis limits after plotting MXUS\n",
        "    y_min, y_max = ax.get_ylim()\n",
        "\n",
        "    # For each anomaly point (Y=1), create a vertical span across the entire plot\n",
        "    for i, (date, is_anomaly) in enumerate(zip(X_df.index, y)):\n",
        "        if is_anomaly == 1:\n",
        "            ax.axvspan(date, date + pd.Timedelta(days=7), alpha=0.3, color='navy', label='Risk-on/Risk-off' if i == 0 else \"\")\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel('Timeline')\n",
        "    ax.set_ylabel('MSCI USA')\n",
        "    ax.set_title('US Equities and risk-on/risk-off periods')\n",
        "\n",
        "    # Add legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    ax.legend(by_label.values(), by_label.keys(), loc='best')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Either 'Y' column or 'MXUS' column is missing in the dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGQAG_xtNXxE"
      },
      "source": [
        "## Stationarity analysis\n",
        "Stationarity is important in order to have a reliable estimation.\\\n",
        "We check it with Augmented Dickey Fuller-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUpsfKb_T4WV",
        "outputId": "c8e5edcf-ec7c-408b-aa4b-3f9b64a0a87f"
      },
      "outputs": [],
      "source": [
        "#Augmented Dickey Fuller test\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "print(\"ADF Test results:\\n\")\n",
        "\n",
        "for col in X_df.columns:\n",
        "    series = X_df[col].dropna()\n",
        "    result = adfuller(series)\n",
        "    p_value = result[1]\n",
        "    print(f\"{col:10s} | ADF Statistic: {result[0]: .4f} | p-value: {p_value:.4f} | {'✔️ Stationary' if p_value < 0.05 else '❌ Non stationary'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPZsBxjvV2ew"
      },
      "source": [
        "By using ADF test, we can notice that three indices are stationary: ECSURPUS, US0001M, VIX. We will cope with non stationarity by applying first differences to indices and currencies and log differences to interest rates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M7DHUDXDUewO"
      },
      "outputs": [],
      "source": [
        "# Make data stationary based on variable type\n",
        "# So we define lists of variables by type\n",
        "indices_currencies = [col for col in X_df.columns if col in [\n",
        "    'XAUBGNL', 'BDIY', 'CRY', 'Cl1', 'DXY', 'EMUSTRUU', 'GBP', 'JPY', 'LF94TRUU',\n",
        "    'LF98TRUU', 'LG30TRUU', 'LMBITR', 'LP01TREU', 'LUACTRUU', 'LUMSTRUU',\n",
        "    'MXBR', 'MXCN', 'MXEU', 'MXIN', 'MXJP', 'MXRU', 'MXUS'\n",
        "]]\n",
        "\n",
        "interest_rates = [col for col in X_df.columns if col in [\n",
        "    'EONIA', 'GTDEM10Y', 'GTDEM2Y', 'GTDEM30Y', 'GTGBP20Y', 'GTGBP2Y', 'GTGBP30Y',\n",
        "    'GTITL10YR', 'GTITL2YR', 'GTITL30YR', 'GTJPY10YR', 'GTJPY2YR', 'GTJPY30YR', 'USGG3M',\n",
        "    'USGG2YR', 'GT10', 'USGG30YR'\n",
        "]]\n",
        "\n",
        "# Create a new dataframe for stationary data\n",
        "stationary_df = pd.DataFrame(index=X_df.index[1:])\n",
        "\n",
        "# Apply log-differences to indices and currencies (always positive)\n",
        "for col in indices_currencies:\n",
        "    if col in X_df.columns:\n",
        "        stationary_df[col] = np.diff(np.log(X_df[col]))\n",
        "\n",
        "# Apply first differences to interest rates (can be negative or very close to 0)\n",
        "for col in interest_rates:\n",
        "    if col in X_df.columns:\n",
        "        stationary_df[col] = np.diff(X_df[col])\n",
        "\n",
        "# Keep ECSURPUS, US0001M, VIX as they are (already stationary)\n",
        "if 'ECSURPUS' in X_df.columns:\n",
        "    stationary_df['ECSURPUS'] = X_df['ECSURPUS'].values[1:]\n",
        "if 'US0001M' in X_df.columns:\n",
        "    stationary_df['US0001M'] = X_df['US0001M'].values[1:]\n",
        "if 'VIX' in X_df.columns:\n",
        "    stationary_df['VIX'] = X_df['VIX'].values[1:]\n",
        "\n",
        "# Adjust the response variable to match the new data length\n",
        "if y is not None:\n",
        "    y_stationary = y[1:]\n",
        "else:\n",
        "    y_stationary = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZzHbFmKUmAB",
        "outputId": "819f3811-0ab8-4988-a054-ff0f3214b1ab"
      },
      "outputs": [],
      "source": [
        "#Augmented Dickey Fuller test : control stationarity after applaying first and log differences\n",
        "print(\"ADF Test results:\\n\")\n",
        "\n",
        "for col in stationary_df.columns:\n",
        "    series = stationary_df[col].dropna()\n",
        "    result = adfuller(series)\n",
        "    p_value = result[1]\n",
        "    print(f\"{col:10s} | ADF Statistic: {result[0]: .4f} | p-value: {p_value:.4f} | {'✔️ Stationary' if p_value < 0.05 else '❌ Non stationary'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mvzSwJwQZf2"
      },
      "source": [
        "Shuffle and split data into training, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ6LIZDEaWYt",
        "outputId": "909be2f2-7b57-4d38-b3d0-7f16c9602d93"
      },
      "outputs": [],
      "source": [
        "# Convert to numpy arrays for easier manipulation\n",
        "X = stationary_df.values\n",
        "y = y_stationary\n",
        "\n",
        "# Step 1: Creating training/cross-validation/test set with reshuffling\n",
        "\n",
        "# Reshuffle the data (this will break down autocorrelation)\n",
        "X_shuffled, y_shuffled = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Separate normal and anomalous examples\n",
        "X_normal = X_shuffled[y_shuffled == 0]\n",
        "X_anomaly = X_shuffled[y_shuffled == 1]\n",
        "\n",
        "# Calculate sizes for each set\n",
        "n_normal = X_normal.shape[0]\n",
        "n_anomaly = X_anomaly.shape[0]\n",
        "\n",
        "# Training set: 80% of normal examples\n",
        "train_size = int(0.8 * n_normal)\n",
        "X_train = X_normal[:train_size]\n",
        "\n",
        "# Cross-validation set: 10% of normal examples and 50% of anomalies\n",
        "cv_normal_size = int(0.1 * n_normal)\n",
        "cv_anomaly_size = int(0.5 * n_anomaly)\n",
        "X_cv_normal = X_normal[train_size:train_size + cv_normal_size]\n",
        "X_cv_anomaly = X_anomaly[:cv_anomaly_size]\n",
        "X_cross_val = np.vstack((X_cv_normal, X_cv_anomaly))\n",
        "y_cross_val = np.hstack((np.zeros(cv_normal_size), np.ones(cv_anomaly_size)))\n",
        "\n",
        "# Test set: 10% of normal examples and 50% of anomalies\n",
        "X_test_normal = X_normal[train_size + cv_normal_size:]\n",
        "X_test_anomaly = X_anomaly[cv_anomaly_size:]\n",
        "X_test = np.vstack((X_test_normal, X_test_anomaly))\n",
        "y_test = np.hstack((np.zeros(len(X_test_normal)), np.ones(len(X_test_anomaly))))\n",
        "\n",
        "# We'll standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_cross_val = scaler.transform(X_cross_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} (all normal)\")\n",
        "print(f\"Cross-validation set size: {X_cross_val.shape[0]} ({cv_normal_size} normal, {cv_anomaly_size} anomalies)\")\n",
        "print(f\"Test set size: {X_test.shape[0]} ({len(X_test_normal)} normal, {len(X_test_anomaly)} anomalies)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0VEsSDWQkOt"
      },
      "source": [
        "## BENCHMARK MODEL\n",
        "\n",
        "### MVG-based anomaly detector with supervised treshold tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "2QndRW58Yr9V",
        "outputId": "2769828b-9459-4b4e-f549-ac009334a81b"
      },
      "outputs": [],
      "source": [
        "# Calculate mean vector\n",
        "mu = np.mean(X_train, axis=0)\n",
        "\n",
        "# Calculate covariance matrix\n",
        "sigma = np.cov(X_train, rowvar=False)\n",
        "\n",
        "print(f\"Mean vector shape: {mu.shape}\")\n",
        "print(f\"Covariance matrix shape: {sigma.shape}\")\n",
        "\n",
        "\n",
        "#Fine tuning the hyperparameter, the threshold ϵ\n",
        "\n",
        "# Function to calculate multivariate Gaussian PDF\n",
        "def multivariate_gaussian_pdf(X, mu, sigma):\n",
        "    \"\"\"Calculate the multivariate Gaussian probability density function\"\"\"\n",
        "    n = mu.shape[0]\n",
        "\n",
        "    # Handle potential numerical issues with the covariance matrix\n",
        "    # Add a small regularization term to ensure positive definiteness\n",
        "    sigma_reg = sigma + np.eye(n) * 1e-8\n",
        "\n",
        "    # Calculate determinant and inverse\n",
        "    try:\n",
        "        det = np.linalg.det(sigma_reg)\n",
        "        inv = np.linalg.inv(sigma_reg)\n",
        "    except np.linalg.LinAlgError:\n",
        "        # If still having issues, use pseudo-inverse\n",
        "        print(\"Warning: Using pseudo-inverse for covariance matrix\")\n",
        "        det = max(np.linalg.det(sigma_reg), 1e-10)\n",
        "        inv = np.linalg.pinv(sigma_reg)\n",
        "\n",
        "    # Calculate PDF for each example\n",
        "    p = np.zeros(X.shape[0])\n",
        "    for i in range(X.shape[0]):\n",
        "        x_mu = X[i] - mu\n",
        "        p[i] = (1.0 / (np.power(2 * np.pi, n/2) * np.sqrt(det))) * \\\n",
        "               np.exp(-0.5 * x_mu.dot(inv).dot(x_mu))\n",
        "\n",
        "    return p\n",
        "\n",
        "# Compute the probability density function for the cross-validation set\n",
        "p_cv = multivariate_gaussian_pdf(X_cross_val, mu, sigma)\n",
        "\n",
        "# Find the range of epsilon values to search\n",
        "min_epsilon = np.min(p_cv)\n",
        "max_epsilon = np.max(p_cv)\n",
        "step_size = (max_epsilon - min_epsilon) / 1000\n",
        "\n",
        "# Find the best epsilon using F1 score\n",
        "best_epsilon = 0\n",
        "best_f1 = 0\n",
        "best_precision = 0\n",
        "best_recall = 0\n",
        "\n",
        "epsilon_values = np.arange(min_epsilon, max_epsilon, step_size)\n",
        "f1_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "\n",
        "for epsilon in epsilon_values:\n",
        "    predictions = (p_cv < epsilon).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_cross_val, predictions, zero_division=0)\n",
        "    recall = recall_score(y_cross_val, predictions, zero_division=0)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    if precision + recall > 0:  # Avoid division by zero\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0\n",
        "\n",
        "    f1_scores.append(f1)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_epsilon = epsilon\n",
        "        best_precision = precision\n",
        "        best_recall = recall\n",
        "\n",
        "print(f\"Best F1 score on CV set: {best_f1:.4f}\")\n",
        "print(f\"Best Epsilon: {best_epsilon:.8e}\")\n",
        "print(f\"Corresponding Precision: {best_precision:.4f}\")\n",
        "print(f\"Corresponding Recall: {best_recall:.4f}\")\n",
        "\n",
        "# Plot F1 score, precision, and recall vs epsilon\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(epsilon_values, f1_scores, 'b-', label='F1 score')\n",
        "plt.plot(epsilon_values, precisions, 'g-', label='Precision')\n",
        "plt.plot(epsilon_values, recalls, 'r-', label='Recall')\n",
        "plt.axvline(x=best_epsilon, color='k', linestyle='--', label=f'Best Epsilon: {best_epsilon:.2e}')\n",
        "plt.xlabel('Epsilon')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Metrics vs. Epsilon value')\n",
        "plt.legend()\n",
        "plt.xscale('log')  # Log scale for better visualization\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAKAgsXoRIGC"
      },
      "source": [
        "Evaluation of results using precision, recall, F1 score and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UavxiwlZe9t-",
        "outputId": "3ba598e1-56f4-4c4d-acfb-ee5b7417765c"
      },
      "outputs": [],
      "source": [
        "# Step 4: Testing the model\n",
        "\n",
        "# Compute the probability density function for the test set\n",
        "p_test = multivariate_gaussian_pdf(X_test, mu, sigma)\n",
        "\n",
        "# Make predictions using the best epsilon\n",
        "predictions = (p_test < best_epsilon).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "MVG_precision = precision_score(y_test, predictions, zero_division=0)\n",
        "MVG_recall = recall_score(y_test, predictions, zero_division=0)\n",
        "MVG_f1 = f1_score(y_test, predictions, zero_division=0)\n",
        "\n",
        "print(\"\\nTest set performance:\")\n",
        "print(f\"Precision: {MVG_precision:.4f}\")\n",
        "print(f\"Recall: {MVG_recall:.4f}\")\n",
        "print(f\"F1 Score: {MVG_f1:.4f}\")\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "print(f\"True Positives: {tp}\")\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Anomaly'],\n",
        "            yticklabels=['Normal', 'Anomaly'],\n",
        "            annot_kws={\"size\": 32})\n",
        "\n",
        "plt.xlabel('Predicted', fontsize=14)\n",
        "plt.ylabel('Actual', fontsize=14)\n",
        "plt.title('Confusion Matrix on Test set', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Visualize examples of correctly and incorrectly classified points\n",
        "# Select two features for visualization\n",
        "if X_test.shape[1] >= 2:\n",
        "    # Choose two features (columns) for visualization\n",
        "    feature1_idx = 22  # First feature\n",
        "    feature2_idx = 15  # Second feature\n",
        "\n",
        "    # Get feature names\n",
        "    feature1_name = stationary_df.columns[feature1_idx]\n",
        "    feature2_name = stationary_df.columns[feature2_idx]\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # True negatives (correctly classified normal points)\n",
        "    plt.scatter(X_test[(y_test == 0) & (predictions == 0), feature1_idx],\n",
        "                X_test[(y_test == 0) & (predictions == 0), feature2_idx],\n",
        "                c='blue', marker='o', alpha=0.5, label='True Negative')\n",
        "\n",
        "    # False positives (normal points classified as anomalies)\n",
        "    plt.scatter(X_test[(y_test == 0) & (predictions == 1), feature1_idx],\n",
        "                X_test[(y_test == 0) & (predictions == 1), feature2_idx],\n",
        "                c='green', marker='o', s=100, edgecolors='k', label='False Positive')\n",
        "\n",
        "    # False negatives (anomalies classified as normal)\n",
        "    plt.scatter(X_test[(y_test == 1) & (predictions == 0), feature1_idx],\n",
        "                X_test[(y_test == 1) & (predictions == 0), feature2_idx],\n",
        "                c='purple', marker='o', s=100, edgecolors='k', label='False Negative')\n",
        "\n",
        "    # True positives (correctly classified anomalies)\n",
        "    plt.scatter(X_test[(y_test == 1) & (predictions == 1), feature1_idx],\n",
        "                X_test[(y_test == 1) & (predictions == 1), feature2_idx],\n",
        "                c='red', marker='o', alpha=0.5, label='True Positive')\n",
        "\n",
        "    plt.xlabel(feature1_name)\n",
        "    plt.ylabel(feature2_name)\n",
        "    plt.title('Classification results on test set')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "    'Precision': MVG_precision,\n",
        "    'Recall': MVG_recall,\n",
        "    'F1 Score': MVG_f1,\n",
        "}, index=['MVG'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaBcjlGgR3TI"
      },
      "source": [
        "### Visualization of MVG anomaly classification on PCA-reduced axis\n",
        "PCA transformation on the test set to reduce the feature space to two dimensions for visual clarity.\n",
        "It then plots each test observation according to:\n",
        "- Prediction outcome: True Positive,\n",
        "False Positive, etc.\n",
        "- Anomaly direction: Anomaly Up, Anomaly Down, Normal\n",
        "\n",
        "\\\\\n",
        "We construct a composite market index based on market capitalization following this logic:\n",
        "- Combine selected regional equity indices weighted by their global market share\n",
        "- The weights reflect approximate market capitalization proportions\n",
        "- A higher WEIGHTED_INDEX indicates stronger aggregate market performance\n",
        "- Subtracting the scaled VIX (volatility index) captures risk-adjusted sentiment\n",
        "\n",
        "Final composite_score = WEIGHTED_INDEX - VIX\n",
        "- Higher values suggest strong, stable market conditions\n",
        "- Lower values suggest elevated volatility or stress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "id": "J3yFM6R2hd1P",
        "outputId": "629d95ef-c138-4d4a-8190-a39aba1ba833"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ----- Step 0: Prepare column index mapping for selected features\n",
        "# Drop non-feature columns ('Y', 'Date') to extract usable market data columns\n",
        "feature_columns = [col for col in stationary_df.columns if col not in ['Y', 'Date']]\n",
        "# Create a dictionary that maps each feature name to its corresponding column index in X_test\n",
        "column_to_index = {col: idx for idx, col in enumerate(feature_columns)}\n",
        "\n",
        "# Construction of a Composite Market Index Based on Market Capitalization\n",
        "# Market cap-based weights for key global indices\n",
        "weights = {\n",
        "    'MXUS': 0.45, 'MXEU': 0.20, 'MXCN': 0.10,\n",
        "    'MXJP': 0.08, 'MXIN': 0.07, 'MXBR': 0.05, 'MXRU': 0.05\n",
        "}\n",
        "\n",
        "# Initialize the index to zero for each observation\n",
        "weighted_index_scaled = np.zeros(X_test.shape[0])\n",
        "\n",
        "# Multiply each selected index by its weight and sum across columns\n",
        "for col, weight in weights.items():\n",
        "    if col in column_to_index:\n",
        "        weighted_index_scaled += X_test[:, column_to_index[col]] * weight\n",
        "\n",
        "# Extract the scaled VIX values from the appropriate column\n",
        "vix_scaled = X_test[:, column_to_index['VIX']]\n",
        "\n",
        "# Final composite score: market strength adjusted for volatility\n",
        "composite_score = weighted_index_scaled - vix_scaled\n",
        "\n",
        "# Determine median value to split positive and negative market signals\n",
        "threshold = np.percentile(composite_score, 50)\n",
        "\n",
        "# Assign market condition labels based on composite score\n",
        "market_condition = []\n",
        "for i, score in enumerate(composite_score):\n",
        "    if y_test[i] == 1:\n",
        "        # Only apply Up/Down classification to actual anomalies\n",
        "        market_condition.append('Anomaly Up' if score > threshold else 'Anomaly Down')\n",
        "    else:\n",
        "        market_condition.append('Normal')  # For normal market observations\n",
        "\n",
        "# --- Step 1: Perform PCA transformation on the test set for visualization ---\n",
        "# Reduce dimensionality to 2D to plot and analyze the structure of model predictions\n",
        "pca = PCA(n_components=2)\n",
        "X_test_pca = pca.fit_transform(X_test)  # Use X_test_scaled here if the model used scaled features\n",
        "\n",
        "# --- Step 2: Set up visual styles for plotting each prediction category ---\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Marker size reflects classification type: larger for positive predictions\n",
        "size_map = {\n",
        "    'True Positive': 60,\n",
        "    'False Positive': 100,\n",
        "    'False Negative': 100,\n",
        "    'True Negative': 60\n",
        "}\n",
        "\n",
        "# Color uniquely identifies each (prediction type, market condition) combination\n",
        "color_map = {\n",
        "    ('True Positive', 'Anomaly Up'): 'darkgreen',\n",
        "    ('True Positive', 'Anomaly Down'): 'salmon',\n",
        "    ('True Positive', 'Normal'): 'lime',\n",
        "\n",
        "    ('False Positive', 'Anomaly Up'): 'orange',\n",
        "    ('False Positive', 'Anomaly Down'): 'red',\n",
        "    ('False Positive', 'Normal'): 'gold',\n",
        "\n",
        "    ('False Negative', 'Anomaly Up'): 'teal',\n",
        "    ('False Negative', 'Anomaly Down'): 'purple',\n",
        "    ('False Negative', 'Normal'): 'pink',\n",
        "\n",
        "    ('True Negative', 'Anomaly Up'): 'gray',  # Rare cases where Normal is misclassified as Up\n",
        "    ('True Negative', 'Anomaly Down'): 'gray',\n",
        "    ('True Negative', 'Normal'): 'gray'\n",
        "}\n",
        "\n",
        "# Helper function to scatter-plot each group with its style and label\n",
        "def plot_group(mask, base_label, condition):\n",
        "    count = np.sum(mask)  # Count number of samples in this group\n",
        "    if count > 0:\n",
        "        label = f\"{base_label} ({condition}) ({count})\"\n",
        "        plt.scatter(X_test_pca[mask, 0],  # Principal Component 1\n",
        "                    X_test_pca[mask, 1],  # Principal Component 2\n",
        "                    label=label,\n",
        "                    c=color_map[(base_label, condition)],\n",
        "                    s=size_map[base_label],\n",
        "                    alpha=0.7,\n",
        "                    edgecolors='k')\n",
        "\n",
        "# --- Step 3: Plot all prediction groups categorized by market condition\n",
        "\n",
        "# Iterate over all possible true/predicted combinations\n",
        "for actual, predicted, base_label in [(0, 0, 'True Negative'),\n",
        "                                      (0, 1, 'False Positive'),\n",
        "                                      (1, 0, 'False Negative'),\n",
        "                                      (1, 1, 'True Positive')]:\n",
        "    # Further subdivide by market anomaly condition\n",
        "    for condition in ['Anomaly Up', 'Anomaly Down', 'Normal']:\n",
        "        mask = (y_test == actual) & (predictions == predicted) & (np.array(market_condition) == condition)\n",
        "        plot_group(mask, base_label, condition)\n",
        "\n",
        "# --- Step 4: Final plot formatting\n",
        "\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"MVG Classification with Anomaly Direction on PCA Axes\")\n",
        "plt.legend(fontsize=10, loc='best')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZFjPxRNhQJg"
      },
      "source": [
        "In the graph above, each (outcome, condition) pair is represented with a unique color, and point size varies by prediction type to highlight detection performance.\n",
        "The legend includes the number of observations for each group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ434bEzUmJ-"
      },
      "source": [
        "## Non linear projection with UMAP\n",
        "UMAP is a nonlinear dimensionality reduction technique that captures manifold structure in the data, we use it to deal with non linearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EJ-xDhfbVtkt",
        "outputId": "ca1f37a3-e1a2-4c12-8d08-1b650e741928"
      },
      "outputs": [],
      "source": [
        "# UMAP visualization of anomalies in 3D\n",
        "\n",
        "# Apply UMAP to reduce dimensionality to 3 components\n",
        "umap_3d = umap.UMAP(n_components=3, random_state=42)\n",
        "X_test_umap = umap_3d.fit_transform(X_test)\n",
        "\n",
        "# Create a DataFrame for easier plotting\n",
        "umap_df = pd.DataFrame(X_test_umap, columns=['UMAP1', 'UMAP2', 'UMAP3'])\n",
        "umap_df['Actual'] = y_test\n",
        "umap_df['Predicted'] = predictions\n",
        "\n",
        "# Create classification categories\n",
        "umap_df['Category'] = 'Unknown'\n",
        "umap_df.loc[(y_test == 0) & (predictions == 0), 'Category'] = 'True Negative'\n",
        "umap_df.loc[(y_test == 0) & (predictions == 1), 'Category'] = 'False Positive'\n",
        "umap_df.loc[(y_test == 1) & (predictions == 0), 'Category'] = 'False Negative'\n",
        "umap_df.loc[(y_test == 1) & (predictions == 1), 'Category'] = 'True Positive'\n",
        "\n",
        "# Define colors, markers, alphas, and sizes\n",
        "colors = {'True Negative': 'gray', 'True Positive': 'black',\n",
        "          'False Positive': 'red', 'False Negative': 'blue'}\n",
        "alphas = {'True Negative': 0.3, 'True Positive': 0.5,\n",
        "          'False Positive': 0.8, 'False Negative': 0.8}\n",
        "sizes = {'True Negative': 30, 'True Positive': 40,\n",
        "         'False Positive': 80, 'False Negative': 80}\n",
        "\n",
        "# Plot the UMAP 3D projection\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot each category\n",
        "for category, group in umap_df.groupby('Category'):\n",
        "    ax.scatter(group['UMAP1'], group['UMAP2'], group['UMAP3'],\n",
        "               color=colors[category],\n",
        "               alpha=alphas[category],\n",
        "               s=sizes[category],\n",
        "               label=f\"{category} ({len(group)})\")\n",
        "\n",
        "# Title and labels\n",
        "ax.set_title('UMAP 3D projection of Anomaly Detection results', fontsize=16)\n",
        "ax.set_xlabel('UMAP1', fontsize=12)\n",
        "ax.set_ylabel('UMAP2', fontsize=12)\n",
        "ax.set_zlabel('UMAP3', fontsize=12)\n",
        "\n",
        "# Legend and layout\n",
        "ax.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr0UahQvXLoI"
      },
      "source": [
        "## SUPERVISED MODELS\n",
        "Models trained using the anomalis lables we have\n",
        "\n",
        "We permormed:\n",
        "- Random Forest with tuned threshold\n",
        "- Random Forest tuned with Optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTtki9EejkQt"
      },
      "source": [
        "Functions and utilities for comparison of supervised and unsupervised Anomaly Detection methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvyv_wSIcA8Q"
      },
      "outputs": [],
      "source": [
        "# Set visualization style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "\n",
        "# We already have X_train, X_cross_val, y_cross_val, X_test, y_test from previous code cells\n",
        "# We'll standardize the data for better performance with many algorithms\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_cv_scaled = scaler.transform(X_cross_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_train_cv = np.vstack((X_train_scaled, X_cv_scaled))\n",
        "y_train_cv = np.hstack((np.zeros(len(X_train_scaled)), y_cross_val))\n",
        "\n",
        "# Function to evaluate and visualize model performance\n",
        "def evaluate_model(y_true, y_pred, y_score, model_name):\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Anomaly'],\n",
        "            yticklabels=['Normal', 'Anomaly'],\n",
        "            annot_kws={\"size\": 32})\n",
        "\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    if y_score is not None:  # Some models don't provide probability scores\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'ROC Curve - {model_name}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Function to visualize results in PCA space\n",
        "def visualize_pca(X, y_true, y_pred, model_name):\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Create a DataFrame for plotting\n",
        "    pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "    pca_df['Actual'] = y_true\n",
        "    pca_df['Predicted'] = y_pred\n",
        "\n",
        "    # Create classification categories\n",
        "    pca_df['Category'] = 'Unknown'\n",
        "    pca_df.loc[(y_true == 0) & (y_pred == 0), 'Category'] = 'True Negative'\n",
        "    pca_df.loc[(y_true == 0) & (y_pred == 1), 'Category'] = 'False Positive'\n",
        "    pca_df.loc[(y_true == 1) & (y_pred == 0), 'Category'] = 'False Negative'\n",
        "    pca_df.loc[(y_true == 1) & (y_pred == 1), 'Category'] = 'True Positive'\n",
        "\n",
        "    # Calculate explained variance\n",
        "    explained_variance = pca.explained_variance_ratio_\n",
        "    total_variance = sum(explained_variance)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Define colors and sizes\n",
        "    colors = {'True Negative': 'gray', 'True Positive': 'black',\n",
        "              'False Positive': 'red', 'False Negative': 'blue'}\n",
        "    alphas = {'True Negative': 0.3, 'True Positive': 0.5,\n",
        "              'False Positive': 0.8, 'False Negative': 0.8}\n",
        "    sizes = {'True Negative': 30, 'True Positive': 40,\n",
        "             'False Positive': 80, 'False Negative': 80}\n",
        "\n",
        "    # Plot each category\n",
        "    for category, group in pca_df.groupby('Category'):\n",
        "        plt.scatter(group['PC1'], group['PC2'],\n",
        "                    color=colors[category],\n",
        "                    alpha=alphas[category],\n",
        "                    s=sizes[category],\n",
        "                    label=f\"{category} ({len(group)})\")\n",
        "\n",
        "    plt.title(f'PCA projection - {model_name}\\nExplained variance: {total_variance:.2%}', fontsize=16)\n",
        "    plt.xlabel(f'PC1 ({explained_variance[0]:.2%})', fontsize=14)\n",
        "    plt.ylabel(f'PC2 ({explained_variance[1]:.2%})', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Store results for comparison\n",
        "results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj5zww-gcqFB"
      },
      "source": [
        "## Random Forest with tuned threshold\n",
        "As threshold of the Random Forest, we choose the one that maximazes the F1-score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D82gEgLSjNPw",
        "outputId": "5cc145d2-f300-407e-8029-b3adcd14f53b"
      },
      "outputs": [],
      "source": [
        "# Training and cross-validation combination\n",
        "X_train_cv = np.vstack((X_train_scaled, X_cv_scaled))\n",
        "y_train_cv = np.hstack((np.zeros(len(X_train_scaled)), y_cross_val))\n",
        "\n",
        "# Model training\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "# Prevision of probabilities on the test set\n",
        "rf_scores = rf_model.predict_proba(X_test_scaled)[:, 1]  # Probabilità di essere anomalia\n",
        "\n",
        "# Threshold tuning to optimize the F1-score\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, rf_scores)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "# Optimal threshold application\n",
        "rf_pred = (rf_scores >= best_threshold).astype(int)\n",
        "\n",
        "# Model valuation\n",
        "rf_metrics = evaluate_model(y_test, rf_pred, rf_scores, \"Random Forest (Supervised, tuned threshold)\")\n",
        "#print(best_threshold)\n",
        "\n",
        "# Save results\n",
        "results.append((\"Random Forest tuned threshold)\", *rf_metrics))\n",
        "\n",
        "metrics_df.loc['Random Forest tuned threshold'] = {'Precision': rf_metrics[0],\n",
        "                                                  'Recall': rf_metrics[1],\n",
        "                                                  'F1 Score': rf_metrics[2]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpdoeeRMdNGl"
      },
      "source": [
        "## Random Forest tuned with Optuna\n",
        "Advanced tuning of Random Forest model with Optuna.\n",
        "\n",
        "Optuna is an automatic hyperparameter optimization framework. It searches for the best combination of hyperparameters to improve a model's performance.\n",
        "It works by:\n",
        "\n",
        "- Defining an objective function that evaluates model performance (MCC-Matthews Correlation Coefficient)\n",
        "\n",
        "- Using intelligent sampling algorithms to explore the hyperparameter space efficiently.\n",
        "\n",
        "- Maximizing the objective function over multiple trials.\n",
        "\n",
        "The threshold is still chosen as the one that maximizes the F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a78f488724cc49de897dfd3b3193944f",
            "5b89f25654724d4386f1404a6e5f633a",
            "6492102a614947c7ac50ff871d7b3bb5",
            "5a161be26ba44f2d996da4e830bf37f9",
            "aa10551466834e8b8a1904484f80a36a",
            "816a0aff13ae43288bfe43885fcdc094",
            "1d426937379742b6881cc20f50cb4c84",
            "aa2d2a19f693498b94a1e5ca703307d9",
            "faabdf581a2c49e0b1b7acc555d60413",
            "30c610c2fcc442e7a3d8f3d5e8ca193f",
            "f5a8b5f127e3404ba43f03570a0388dd"
          ]
        },
        "id": "MSX219HLYiPq",
        "outputId": "afa5be55-0968-4534-d0a0-09931d65c9d8"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Optuna tuning (sulla MCC)\n",
        "# ----------------------------\n",
        "def objective_rf(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 50)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5, 0.7, None])\n",
        "    class_weight = trial.suggest_categorical('class_weight', ['balanced', None])\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        class_weight=class_weight,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(rf, X_train_cv, y_train_cv,\n",
        "                            cv=5, scoring=make_scorer(matthews_corrcoef), n_jobs=-1)\n",
        "    return score.mean()\n",
        "\n",
        "study_rf = optuna.create_study(direction='maximize')\n",
        "study_rf.optimize(objective_rf, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"Migliori parametri RF:\", study_rf.best_params)\n",
        "\n",
        "# ----------------------------\n",
        "# Final train optimal parameters\n",
        "# ----------------------------\n",
        "best_rf = RandomForestClassifier(\n",
        "    **study_rf.best_params,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "best_rf.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "# ----------------------------\n",
        "# Predict proba e tuning treshold\n",
        "# ----------------------------\n",
        "rf_scores = best_rf.predict_proba(X_test_scaled)[:, 1]  # Probabilità di classe 1\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, rf_scores)\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
        "best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "print(f\"Soglia ottimale F1-score: {best_threshold:.4f}\")\n",
        "\n",
        "# Predizione finale con soglia ottimale\n",
        "rf_pred = (rf_scores >= best_threshold).astype(int)\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation\n",
        "# ----------------------------\n",
        "rf_metrics = evaluate_model(y_test, rf_pred, rf_scores, \"Random Forest (Tuned + Threshold)\")\n",
        "results.append((\"Random Forest (Tuned Threshold + Parameters)\", *rf_metrics))\n",
        "\n",
        "metrics_df.loc['Random Forest (Tuned Threshold + Parameters)'] = {\n",
        "    'Precision': rf_metrics[0],\n",
        "    'Recall': rf_metrics[1],\n",
        "    'F1 Score': rf_metrics[2]\n",
        "}\n",
        "\n",
        "mcc = matthews_corrcoef(y_test, rf_pred)\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O3YFpgnERDT"
      },
      "source": [
        "# Elliptic Envelope\n",
        "This method tries to fit a Guassian distribution to data computing the sample mean and the sample covariance matrix. Given the expected rate of outliers as input (*anomaly rate*), it returns the ellipsoid centered in the mean than collects (1-*anomaly rate*)% of the original data. The threshold is computed using the Mahalanobis distance, whose distribution under normality assumption is given by a Chi-squared with **p** degrees of freedom, where **p** is the dimensionality of the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMpvBa05E9UT",
        "outputId": "1158b7ee-6bb5-45b3-d899-7736f6fe3821"
      },
      "outputs": [],
      "source": [
        "# Set visualization style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} (all normal)\")\n",
        "print(f\"Cross-validation set size: {X_cross_val.shape[0]} ({cv_normal_size} normal, {cv_anomaly_size} anomalies)\")\n",
        "print(f\"Test set size: {X_test.shape[0]} ({len(X_test_normal)} normal, {len(X_test_anomaly)} anomalies)\")\n",
        "print(\"Test shape\", X_test.shape)\n",
        "print(\"y object\", type(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3Y3BQrVFBIg"
      },
      "source": [
        "## Function definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBDKBsX6bNoZ"
      },
      "source": [
        "Before applying the elliptic algortihm, we define some functions that we will apply in the next sections\n",
        "\n",
        "Function implementation:\n",
        "\n",
        "Given a sample $X_1, \\ldots, X_n \\overset{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\Sigma)$, with $\\mu \\in \\mathbb{R}^p$ and $\\Sigma \\in \\mathbb{R}^{p \\times p}$, the Mahalanobis distance of a point $x$ from the mean $\\mu$ is induced by the covariance matrix $\\Sigma$, is defined as:\n",
        "$d^2(x, \\mu) = (x - \\mu)' \\Sigma^{-1} (x - \\mu)$\n",
        "\n",
        "Unlike the Euclidean distance, the Mahalanobis distance takes into account the covariance structure that generates the data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBHKw-u4FEZ8"
      },
      "outputs": [],
      "source": [
        "# Differently from the Euclidean distance, the Mahalanobis distance takes into account the covariance\n",
        "# structure that generates data.\n",
        "def mahalanobis_distance(X_df):\n",
        "    \"\"\"\n",
        "    Computes the Mahalanobis distance of each unit of the dataset passed as input\n",
        "\n",
        "    Args:\n",
        "        X_df (DataFrame): dataset\n",
        "\n",
        "    Returns:\n",
        "        numpy array: Mahalanobis distance vector\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    Ritorna un Numpy Array\n",
        "    \"\"\"\n",
        "    X_df_centered = X_df - np.mean(X_df, axis=0)\n",
        "    cov = np.cov(X_df, rowvar=False)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    mahal_dist = []\n",
        "    for i in range(X_df.shape[0]):\n",
        "        mahal_dist.append(np.sqrt(X_df_centered[i].dot(inv_cov).dot(X_df_centered[i])))\n",
        "    return np.array(mahal_dist)\n",
        "\n",
        "\n",
        "def print_evaluation(y, y_pred, title):\n",
        "    \"\"\"\n",
        "    Auxiliary function that plots the confusion matrix of a classifier. It\n",
        "    also returns some evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "       y: true labels.\n",
        "       y: predicted labels.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{title}:\")\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    print(\"Matrice di confusione:\")\n",
        "    print(cm)\n",
        "\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn) # Ratio of correct predictions\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0 # Correct positive predictions over the total positive ones\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0 # Percentage of positive values identified\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0 # F-score\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VNwpbUAbwUL"
      },
      "source": [
        "## Gaussianity check\n",
        "**Univariate case**\n",
        "\n",
        "Even if Shapiro-Wilk test is widely used in statistics, it has some criticalities:\n",
        "\n",
        "\n",
        "*   it can be applied if the sample size n is between 3 and 5000\n",
        "*   it is very sensitive to deviation from normality\n",
        "\n",
        "That's why, we will compare it with the results given by Jera-Barque test, which is more suited to financial series since it takes into account both kurtosis and skewness.\n",
        "\n",
        "**Multivariate case**\n",
        "\n",
        "To check if gaussianity is satisfied, we will compare the Malahanobis distance of the points in our dataset with the theoretical quantiles of a Chi-squared distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnBeNhLiFK3Q",
        "outputId": "bc15113f-171b-45c3-e429-e307c9284be3"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Convert 'X_test' and 'y_test' to DataFrame type\n",
        "X_test_df = pd.DataFrame(X_test, columns=X_df.columns)\n",
        "y = pd.DataFrame(y_test, columns=['Y_Flag'])\n",
        "\n",
        "# Create a DataFrame in which we save the test results\n",
        "print(\"\\n--- Univariate Gaussianity Test ---\\n\")\n",
        "shapiro_results = pd.DataFrame(columns=['Indice', 'Statistics', 'P-value', 'Normal'])\n",
        "bj_results = pd.DataFrame(columns=['Indice', 'Statistics', 'P-value', 'Normal'])\n",
        "\n",
        "for col in X_test_df.columns:  # Iterate over the indeces (i.e over the columns of the dataset)\n",
        "    # Shapiro - Wilk Test\n",
        "    stat, p = stats.shapiro(X_test_df[col])\n",
        "    shapiro_results.loc[len(shapiro_results)] = [col, stat, p, p > 0.05]  # Save the result in the first line available\n",
        "    # Bera - Jarque Test\n",
        "    stat, p = stats.jarque_bera(X_test_df[col])\n",
        "    bj_results.loc[len(bj_results)] = [col, stat, p, p > 0.05]\n",
        "\n",
        "# Print results\n",
        "print(\n",
        "    f\"\\nIndices with a gaussian distribution [S-W]: {shapiro_results['Normal'].sum()} su {len(shapiro_results)}\")\n",
        "print(f\"\\nIndices with a gaussian distribution [B-J]: {bj_results['Normal'].sum()} su {len(bj_results)}\")\n",
        "normality_results = pd.concat([shapiro_results, bj_results], axis=0)\n",
        "normality_results.to_excel(\"Normality_results.xlsx\", index=True)\n",
        "\n",
        "# 2.3 Test di normalità multivariata\n",
        "print(\"\\n--- Multivariate Gaussianity Test ---\\n\")\n",
        "\n",
        "# Standardizziamo i dati prima di calcolare la distanza di Mahalanobis\n",
        "# X_df_std = StandardScaler().fit_transform(X_df) # I dati sono già standardizzati\n",
        "mahal_dist = mahalanobis_distance(X_test_df.values)\n",
        "\n",
        "# Recall that in gaussian framework, the square of the Mahalanobis distance is a Chi-squared with p degrees of freedom\n",
        "chi2_quantiles = stats.chi2.ppf(np.arange(0.01, 1, 0.01), df=X_test_df.shape[1])\n",
        "mahal_dist_sorted = np.sort(mahal_dist ** 2)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(chi2_quantiles, mahal_dist_sorted[:len(chi2_quantiles)], alpha=0.6)\n",
        "plt.plot([0, max(chi2_quantiles)], [0, max(chi2_quantiles)], 'r--')\n",
        "plt.xlabel('Theoretical quantiles')\n",
        "plt.ylabel('Sample quantiles')\n",
        "plt.title('QQ-Plot to assess multivariate gaussianity')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.close()\n",
        "\n",
        "print(\n",
        "    \"If the points deviate significantly from the red line, the data do not follow a multivariate normal distribution\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xGBzTJQFQqa"
      },
      "source": [
        "From the plot it is clearly visible that the data are not gaussian at all since points are located way below the theoretical line.\n",
        "\n",
        "To tackle this issue, we try to apply **PowerTransformer** to make data more gaussian-like. In particular, we resort to the *Yeo-Johnson* method which is able to deal with both positive and negative data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6a7FnY2cSaD",
        "outputId": "a3bbf590-6290-4254-d4ec-a3be96ea5a6c"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- TRASFORMATIONS ---\\n\")\n",
        "print(\"Applying PowerTransformer...\\n\")\n",
        "\n",
        "# Apply PowerTransformer al dataset X_test_df\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "X_test_tran = pt.fit_transform(X_test_df)\n",
        "\n",
        "# Convert X_test_tran from ndarray to Dataframe\n",
        "X_test_tran_df = pd.DataFrame(X_test_tran, columns=X_test_df.columns, index=X_test_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVUK_jsGFT16",
        "outputId": "259bc75b-1a44-47da-9200-1778a1b31a1c"
      },
      "outputs": [],
      "source": [
        "# Let's repeat the gaussianity tests again\n",
        "\n",
        "print(\"\\n--- Univariate Gaussianity Test ---\\n\")\n",
        "# Clean dataframes containing the results about S-W and B-J tests we did before\n",
        "#print(\"Shapiro pre pulizia:\", shapiro_results.shape)\n",
        "shapiro_results = pd.DataFrame(columns = shapiro_results.columns)\n",
        "bj_results = pd.DataFrame(columns = bj_results.columns)\n",
        "#print(\"Shapiro post pulizia:\", shapiro_results.shape)\n",
        "\n",
        "for col in X_test_tran_df.columns: # Iterate over the indeces (i.e over the columns of the dataset)\n",
        "    # Shapiro - Wilk Test\n",
        "    stat, p = stats.shapiro(X_test_tran_df[col])\n",
        "    shapiro_results.loc[len(shapiro_results)] = [col, stat, p, p > 0.05]  # Save the result into the first available line\n",
        "    # Bera - Jarque Test\n",
        "    stat, p = stats.jarque_bera(X_test_tran_df[col])\n",
        "    bj_results.loc[len(bj_results)] = [col, stat, p, p > 0.05]\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nIndices with a gaussian distribution [S-W]: {shapiro_results['Normal'].sum()} su {len(shapiro_results)}\")\n",
        "print(f\"\\nIndices with a gaussian distribution [B-J]: {bj_results['Normal'].sum()} su {len(bj_results)}\")\n",
        "\n",
        "normality_results = pd.concat([shapiro_results, bj_results], axis=0)\n",
        "normality_results.to_excel(\"Normality_results_post.xlsx\", index = True)\n",
        "\n",
        "# 2.3 Test di normalità multivariata (approssimazione semplificata)\n",
        "print(\"\\n--- MULTIVARIATE NORMALITY TEST ---\\n\")\n",
        "\n",
        "# Standardizziamo i dati prima di calcolare la distanza di Mahalanobis\n",
        "#X_test_tran_df_std = StandardScaler().fit_transform(X_test_tran_df)\n",
        "mahal_dist = mahalanobis_distance(X_test_tran_df.values)\n",
        "\n",
        "# Recall that in gaussian framework, the square of the Mahalanobis distance is a Chi-squared with p degrees of freedom\n",
        "chi2_quantiles = stats.chi2.ppf(np.arange(0.01, 1, 0.01), df=X_test_tran_df.shape[1])\n",
        "mahal_dist_sorted = np.sort(mahal_dist**2)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(chi2_quantiles, mahal_dist_sorted[:len(chi2_quantiles)], alpha=0.6)\n",
        "plt.plot([0, max(chi2_quantiles)], [0, max(chi2_quantiles)], 'r--')\n",
        "plt.xlabel('Theoretical Chi-squared quantiles')\n",
        "plt.ylabel('Empirical quantiles of the Mahalanobis squared distances')\n",
        "plt.title('QQ-Plot for assessing multivariate normality')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uYki_5idm2e"
      },
      "source": [
        "It is visible that the transformation is not enough to recover the normality assumption. This is probabibly due to the high-dimensionality of the embedding space.\n",
        "\n",
        "Well aware of this problem, we try to apply the Elliptic envelope anyway. Given that the test set contains more than 50% of anomalies, we pass 0.5 as input in the Elliptic Envelope initialization (max. value allowed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "80dqmGjAFYpf",
        "outputId": "333a4ab1-f570-4f7b-e628-d98cef0d6166"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- ELLIPTIC ENVELOPE ---\\n\")\n",
        "\n",
        "# Compute the anomaly rate of our test set\n",
        "anomaly_rate = y.mean().iloc[0]\n",
        "print(f\"Percentage of anomalies in the test set: {anomaly_rate:.4f} ({int(anomaly_rate*len(X_test_df))} su {len(X_test_df)})\")\n",
        "#\n",
        "# Set the contamination parameter according to the percentage of existing anomalies\n",
        "contamination = anomaly_rate\n",
        "\n",
        "# Elliptic Envelope application with StandardScaler\n",
        "envelope_std = EllipticEnvelope(contamination=0.5, random_state=42)\n",
        "y_pred_std = envelope_std.fit_predict(X_test_tran_df)\n",
        "# Convert to binary format (1 for inlier, -1 for outlier → 0 for normal, 1 for anomaly)\n",
        "y_pred_std_binary = np.where(y_pred_std == -1, 1, 0)\n",
        "\n",
        "# The decision_function() method of the EllipticEnvelope object returns a measure of the distance\n",
        "# of each point from the center of the elliptical distribution. The more negative the value, the more likely it is to be an outlier.\n",
        "# By prepending a minus sign, we obtain that: higher values indicate a higher likelihood of being an anomaly.\n",
        "\n",
        "anomaly_scores_std = -envelope_std.decision_function(X_test_tran_df)\n",
        "\n",
        "# 5. Evaluation and comparison with the original labels\n",
        "print(\"\\n--- RESULT VALUATION ---\\n\")\n",
        "\n",
        "# Convert y to binary numeric format if necessary\n",
        "y_binary = y.astype(int)\n",
        "\n",
        "# DataFrame\n",
        "results = pd.DataFrame({\n",
        "    'True_Anomaly': y_binary.values.ravel(),\n",
        "    'Predicted_Std': y_pred_std_binary,\n",
        "    'Score_Std': anomaly_scores_std,\n",
        "})\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print_evaluation(y_binary, y_pred_std_binary, \"StandardScaler valuation\")\n",
        "\n",
        "metrics_df.loc['Elliptic Envelope (StandardScaler)'] = {\n",
        "    'Precision': precision_score(y_binary, y_pred_std_binary, zero_division=0),\n",
        "    'Recall': recall_score(y_binary, y_pred_std_binary, zero_division=0),\n",
        "    'F1 Score': f1_score(y_binary, y_pred_std_binary, zero_division=0)\n",
        "}\n",
        "\n",
        "# Results\n",
        "print(\"\\n--- RESULTS VALUATION ---\\n\")\n",
        "\n",
        "mahalanobis_envelope = envelope_std.mahalanobis(X_test_tran_df)\n",
        "# Threshold\n",
        "threshold = np.percentile(mahalanobis_envelope, 100 * (1 - envelope_std.contamination))\n",
        "\n",
        "# 6.1 Comparison between true and predicted anomalies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(results)), results['Score_Std'], c=results['True_Anomaly'],\n",
        "           alpha=0.6, edgecolor='k', s=50, cmap='coolwarm')\n",
        "\n",
        "plt.axhline(y=threshold, color='r', linestyle='--',\n",
        "           label=f'Decision threshold: {threshold:.4f}')\n",
        "\n",
        "plt.xlabel('Observation index')\n",
        "plt.ylabel('Anomaly score (StandardScaler)')\n",
        "plt.title('Comparison between true anomalies and anomaly scores')\n",
        "plt.colorbar(label='True anomaly (1=Yes, 0=No)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('anomalies_comaprison.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"Comparison graph saves as 'anomalies_comparison.png'\")\n",
        "\n",
        "# 6.2 Error analysis\n",
        "# Identify false positives and false negatives\n",
        "y_binary_array = y_binary.values.flatten()\n",
        "y_pred_array = y_pred_std_binary.flatten()\n",
        "fp_indices = np.where((y_binary_array == 0) & (y_pred_array == 1))[0]\n",
        "fn_indices = np.where((y_binary_array == 1) & (y_pred_array == 0))[0]\n",
        "\n",
        "print(f\"\\nFalse positives: {len(fp_indices)} observations\")\n",
        "print(f\"False negatives: {len(fn_indices)} observations\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_binary_array, y_pred_std_binary)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "             xticklabels=['Normal', 'Anomaly'],\n",
        "            yticklabels=['Normal', 'Anomaly'],\n",
        "            annot_kws={\"size\": 32})  # Increase the font size here\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Elliptic Envelope')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdqCT60LFke4",
        "outputId": "f15cc11f-9ed6-49e5-9427-b374ebbb5f26"
      },
      "outputs": [],
      "source": [
        "def plot_pca_results(X_data, y_true, y_pred, title='PCA with errors classification'):\n",
        "    \"\"\"\n",
        "    Visualize the observations in the plane of the first two principal components,\n",
        "    highlighting TP, TN, FP, and FN with different colors.\n",
        "\n",
        "    Parameters:\n",
        "    X_data (DataFrame or ndarray): Input data\n",
        "    y_true (array): True labels (1 for anomaly, 0 for normal)\n",
        "    y_pred (array): Predicted labels (1 for anomaly, 0 for normal)\n",
        "    title (str): Title of the plot\n",
        "\n",
        "    Returns:\n",
        "    None: Saves the plot as 'pca_classification_results.png'\n",
        "    \"\"\"\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    X = np.array(X_data)\n",
        "    y_true = np.array(y_true).flatten()\n",
        "    y_pred = np.array(y_pred).flatten()\n",
        "\n",
        "    # PCA for 2 dimensions reduction\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "  # Create error categories\n",
        "  # TP: True anomalies correctly identified (y_true=1, y_pred=1)\n",
        "  # TN: Normals correctly identified (y_true=0, y_pred=0)\n",
        "  # FP: Normals incorrectly identified as anomalies (y_true=0, y_pred=1)\n",
        "  # FN: Anomalies incorrectly identified as normal (y_true=1, y_pred=0)\n",
        "\n",
        "    tp_indices = np.where((y_true == 1) & (y_pred == 1))[0]\n",
        "    tn_indices = np.where((y_true == 0) & (y_pred == 0))[0]\n",
        "    fp_indices = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "    fn_indices = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "\n",
        "    colors = {\n",
        "        'TP': '#2ca02c',  # Green\n",
        "        'TN': '#1f77b4',  # Blue\n",
        "        'FP': '#ff7f0e',  # Orange\n",
        "        'FN': '#d62728'   # Red\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Add information for each category\n",
        "    plt.scatter(X_pca[tn_indices, 0], X_pca[tn_indices, 1], c=colors['TN'],\n",
        "                label=f'TN (True negatives): {len(tn_indices)}', alpha=0.7, s=50, edgecolor='k')\n",
        "    plt.scatter(X_pca[tp_indices, 0], X_pca[tp_indices, 1], c=colors['TP'],\n",
        "                label=f'TP (True positives): {len(tp_indices)}', alpha=0.7, s=50, edgecolor='k')\n",
        "    plt.scatter(X_pca[fp_indices, 0], X_pca[fp_indices, 1], c=colors['FP'],\n",
        "                label=f'FP (False positives): {len(fp_indices)}', alpha=0.7, s=50, edgecolor='k')\n",
        "    plt.scatter(X_pca[fn_indices, 0], X_pca[fn_indices, 1], c=colors['FN'],\n",
        "                label=f'FN (False negatives): {len(fn_indices)}', alpha=0.7, s=50, edgecolor='k')\n",
        "\n",
        "    # Aggiungi informazioni sul grafico\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel(f'First principal component ({pca.explained_variance_ratio_[0]:.2%} variance explained)', fontsize=12)\n",
        "    plt.ylabel(f'Second principal component ({pca.explained_variance_ratio_[1]:.2%} variance explained)', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(fontsize=10, loc='best')\n",
        "\n",
        "    # Aggiungi annotazioni sulle performance\n",
        "    accuracy = (len(tp_indices) + len(tn_indices)) / len(y_true)\n",
        "    precision = len(tp_indices) / (len(tp_indices) + len(fp_indices)) if (len(tp_indices) + len(fp_indices)) > 0 else 0\n",
        "    recall = len(tp_indices) / (len(tp_indices) + len(fn_indices)) if (len(tp_indices) + len(fn_indices)) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    plt.figtext(0.02, 0.02, f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}',\n",
        "                fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Salva il grafico\n",
        "    plt.tight_layout()\n",
        "    plt.close()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Ritorni anche le varianze spiegate\n",
        "    return pca.explained_variance_ratio_\n",
        "\n",
        "plot_pca_results(X_test_tran_df, y_binary, y_pred_std_binary,\n",
        "                'EllipticEnvelope classification in the PCA plane')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRjZh1E7kiRi"
      },
      "source": [
        "## Autoencoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnnO6iO9kmao",
        "outputId": "d78e1b07-d2ef-4c65-f5de-f778f1fde1db"
      },
      "outputs": [],
      "source": [
        "# 3.  Deep Learning methods: Autoencoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "X_cv_tensor = torch.FloatTensor(X_cv_scaled).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)  # Input = Output for autoencoder\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the Autoencoder architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim=16):\n",
        "        super(Autoencoder, self).__init__()\n",
        "                # Calculate layer sizes for a gradually decreasing architecture\n",
        "        layer1_size = int(input_dim * (3/4))\n",
        "        layer2_size = int(layer1_size * (3/4))\n",
        "        layer3_size = int(layer2_size * (3/4))\n",
        "        layer4_size = int(layer3_size * (3/4))\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, layer1_size),\n",
        "            nn.BatchNorm1d(layer1_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(layer1_size, layer2_size),\n",
        "            nn.BatchNorm1d(layer2_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(layer2_size, layer3_size),\n",
        "            nn.BatchNorm1d(layer3_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(layer3_size, layer4_size),\n",
        "            nn.BatchNorm1d(layer4_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(layer4_size, encoding_dim),\n",
        "            nn.BatchNorm1d(encoding_dim),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, layer4_size),\n",
        "            nn.BatchNorm1d(layer4_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(layer4_size, layer3_size),\n",
        "            nn.BatchNorm1d(layer3_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(layer3_size, layer2_size),\n",
        "            nn.BatchNorm1d(layer2_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(layer2_size, layer1_size),\n",
        "            nn.BatchNorm1d(layer1_size),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(layer1_size, input_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Get input dimension from data\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "print(f\"Input dimension: {input_dim}\")\n",
        "\n",
        "# Create the autoencoder model\n",
        "encoding_dim = min(16, input_dim // 4)\n",
        "autoencoder_model = Autoencoder(input_dim, encoding_dim).to(device)\n",
        "print(f\"Encoding dimension: {encoding_dim}\")\n",
        "print(autoencoder_model)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# Training function\n",
        "def train_autoencoder(model, train_loader, num_epochs=500, patience=20):\n",
        "    # For early stopping\n",
        "    best_loss = float('inf')\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    # For plotting\n",
        "    train_losses = []\n",
        "\n",
        "    print(\"Training autoencoder...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        autoencoder_model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for data, _ in train_loader:\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = autoencoder_model(data)\n",
        "            loss = criterion(outputs, data)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "\n",
        "        # Calculate epoch loss\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}')\n",
        "        if epoch_loss < best_loss:\n",
        "         best_loss = epoch_loss\n",
        "         no_improve_epochs = 0\n",
        "         torch.save(model.state_dict(), 'best_autoencoder.pth')\n",
        "        else:\n",
        "         no_improve_epochs += 1\n",
        "         if no_improve_epochs >= patience:\n",
        "           print(f'Early stopping at epoch {epoch+1}')\n",
        "           break\n",
        "\n",
        "\n",
        "    # Load the best model\n",
        "    autoencoder_model.load_state_dict(torch.load('best_autoencoder.pth'))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses)\n",
        "    plt.title('Autoencoder Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return autoencoder_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EbrBWzGPlhsV",
        "outputId": "24dc508f-6bd9-45a8-f3b8-bf1cbb36017a"
      },
      "outputs": [],
      "source": [
        "# Train the autoencoder\n",
        "autoencoder_model = train_autoencoder(autoencoder_model, train_loader, num_epochs=500, patience=20)\n",
        "\n",
        "# Function to compute reconstruction error\n",
        "def compute_reconstruction_error(model, data_tensor):\n",
        "    autoencoder_model.eval()\n",
        "    with torch.no_grad():\n",
        "        reconstructions = autoencoder_model(data_tensor)\n",
        "        # Compute MSE for each sample\n",
        "        mse = ((reconstructions - data_tensor) ** 2).mean(dim=1).cpu().numpy()\n",
        "    return mse\n",
        "\n",
        "# Compute reconstruction errors\n",
        "train_errors = compute_reconstruction_error(autoencoder_model, X_train_tensor)\n",
        "cv_errors = compute_reconstruction_error(autoencoder_model, X_cv_tensor)\n",
        "test_errors = compute_reconstruction_error(autoencoder_model, X_test_tensor)\n",
        "\n",
        "# Determine threshold for anomaly detection using cross-validation set\n",
        "# We'll use the contamination rate from the cross-validation set\n",
        "contamination = np.mean(y_cross_val)\n",
        "print(f\"Contamination rate from cross-validation set: {contamination:.4f}\")\n",
        "\n",
        "# Find the threshold that best separates normal and anomalous examples in the CV set\n",
        "cv_errors_normal = cv_errors[y_cross_val == 0]\n",
        "cv_errors_anomaly = cv_errors[y_cross_val == 1]\n",
        "\n",
        "# Plot histogram of reconstruction errors by class\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(cv_errors_normal, bins=50, alpha=0.5, label='Normal', color='blue')\n",
        "plt.hist(cv_errors_anomaly, bins=50, alpha=0.5, label='Anomaly', color='red')\n",
        "plt.axvline(x=np.percentile(cv_errors, 100 * (1 - contamination)),\n",
        "            color='green', linestyle='dashed', linewidth=2,\n",
        "            label=f'Threshold at {100 * (1 - contamination):.1f}th percentile')\n",
        "plt.title('Reconstruction error distribution by class (Cross-Validation Set)')\n",
        "plt.xlabel('Reconstruction error (MSE)')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Set threshold at the percentile corresponding to the contamination rate\n",
        "threshold = np.percentile(cv_errors, 100 * (1 - contamination))\n",
        "print(f\"Threshold for anomaly detection: {threshold:.6f}\")\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = (test_errors > threshold).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "olF-xHvrli02",
        "outputId": "91383cd0-0374-4103-9c8e-040bfd204d65"
      },
      "outputs": [],
      "source": [
        "# Evaluate the autoencoder model\n",
        "def evaluate_autoencoder(y_true, y_pred, reconstruction_errors, threshold):\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(\"\\nAutoencoder performance:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Visualize confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Anomaly'],\n",
        "            yticklabels=['Normal', 'Anomaly'],\n",
        "            annot_kws={\"size\": 32})\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix - Autoencoder')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, reconstruction_errors)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC curve - Autoencoder')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate autoencoder performance\n",
        "ae_metrics = evaluate_autoencoder(y_test, y_pred, test_errors, threshold)\n",
        "\n",
        "metrics_df.loc['Autoencoder'] = {\n",
        "    'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "    'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "    'F1 Score': f1_score(y_test, y_pred, zero_division=0)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJNo8FV8Su9u"
      },
      "source": [
        "## LSTM AUTOENCODER\n",
        "Long Short Term Autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tPz_OD-fRkFi",
        "outputId": "dc5c2e53-9f47-4f90-8be2-d53cce612fe4"
      },
      "outputs": [],
      "source": [
        "seed = 12\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# sliding window\n",
        "def create_sliding_windows(X, window_size):\n",
        "    X_windows = []\n",
        "    for i in range(X.shape[0] - window_size + 1):\n",
        "        X_windows.append(X[i:i+window_size])\n",
        "    return np.array(X_windows)\n",
        "\n",
        "# Dataset with sliding windows\n",
        "window_size = 20  # numero di giorni nella sequenza\n",
        "X_train_windows = create_sliding_windows(X_train_scaled, window_size)\n",
        "X_cv_windows = create_sliding_windows(X_cv_scaled, window_size)\n",
        "X_test_windows = create_sliding_windows(X_test_scaled, window_size)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_windows).float().to(device)\n",
        "X_cv_tensor = torch.tensor(X_cv_windows).float().to(device)\n",
        "X_test_tensor = torch.tensor(X_test_windows).float().to(device)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_tensor), batch_size=32, shuffle=True)\n",
        "\n",
        "# LSTM Autoencoder\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, latent_dim=16, num_layers=2, dropout=0.3):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.latent = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.decoder_input = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.decoder_lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        _, (h_n, _) = self.encoder_lstm(x)\n",
        "        h_n = h_n[-1]\n",
        "        z = self.latent(h_n)\n",
        "        decoder_input = self.decoder_input(z).unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        decoded_seq, _ = self.decoder_lstm(decoder_input)\n",
        "        output = self.output_layer(decoded_seq)\n",
        "        return output, z\n",
        "\n",
        "#Training with Early Stopping\n",
        "def reconstruction_loss(x, x_recon):\n",
        "    return torch.mean((x - x_recon) ** 2)\n",
        "\n",
        "def train_lstm_autoencoder(model, train_loader, num_epochs=200, lr=1e-3, patience=10):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    best_loss = float('inf')\n",
        "    no_improve_epochs = 0\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x_batch = batch[0]\n",
        "            optimizer.zero_grad()\n",
        "            x_recon, _ = model(x_batch)\n",
        "            loss = reconstruction_loss(x_batch, x_recon)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            no_improve_epochs = 0\n",
        "            torch.save(model.state_dict(), 'lstm_autoencoder.pth')\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            if no_improve_epochs >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load('lstm_autoencoder.pth'))\n",
        "\n",
        "    plt.plot(losses)\n",
        "    plt.title(\"LSTM Autoencoder Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MSE Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "model = LSTMAutoencoder(input_dim=42, hidden_dim=64, latent_dim=16).to(device)\n",
        "model = train_lstm_autoencoder(model, train_loader, num_epochs=200, lr=1e-3, patience=10)\n",
        "\n",
        "#Anomaly Detection\n",
        "def compute_reconstruction_error(model, data_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_recon, _ = model(data_tensor)\n",
        "        errors = ((x_recon - data_tensor) ** 2).mean(dim=(1,2)).cpu().numpy()\n",
        "    return errors\n",
        "\n",
        "cv_errors = compute_reconstruction_error(model, X_cv_tensor)\n",
        "test_errors = compute_reconstruction_error(model, X_test_tensor)\n",
        "\n",
        "# Cutting labels for alignment\n",
        "y_cross_val_trimmed = y_cross_val[window_size - 1:]\n",
        "y_test_trimmed = y_test[window_size - 1:]\n",
        "\n",
        "#Threshold with contamination\n",
        "contamination = np.mean(y_cross_val_trimmed)\n",
        "threshold = np.percentile(cv_errors, 100 * (1 - contamination))\n",
        "print(f\"Contamination rate: {contamination:.4f}\")\n",
        "print(f\"Threshold: {threshold:.6f}\")\n",
        "\n",
        "# Final predictions\n",
        "y_pred = (test_errors > threshold).astype(int)\n",
        "\n",
        "# Metrics\n",
        "print(\"\\n--- LSTM Autoencoder Anomaly Detection ---\")\n",
        "print(f\"Accuracy : {accuracy_score(y_test_trimmed, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_trimmed, y_pred):.4f}\")\n",
        "print(f\"Recall   : {recall_score(y_test_trimmed, y_pred):.4f}\")\n",
        "print(f\"F1 Score : {f1_score(y_test_trimmed, y_pred):.4f}\")\n",
        "\n",
        "metrics_df.loc['LSTM Autoencoder'] = {\n",
        "    'Precision': precision_score(y_test_trimmed, y_pred, zero_division=0),\n",
        "    'Recall': recall_score(y_test_trimmed, y_pred, zero_division=0),\n",
        "    'F1 Score': f1_score(y_test_trimmed, y_pred, zero_division=0)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "LB2gjSWtRzMH",
        "outputId": "e71cf3bd-206f-47fc-847a-e52e0bd3293f"
      },
      "outputs": [],
      "source": [
        " # Confusion Matrix\n",
        "cm = confusion_matrix(y_test_trimmed, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Anomaly'],\n",
        "            yticklabels=['Normal', 'Anomaly'],\n",
        "            annot_kws={\"size\": 32})\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - LSTM Autoencoder')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPHjgC6ovYKh"
      },
      "source": [
        "## LSTM Autoencoder predictions with market anomaly direction (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "id": "5iPB_AsnifmK",
        "outputId": "5e3d507e-4c43-4be9-bef2-ce1039754524"
      },
      "outputs": [],
      "source": [
        "# --- Market-based Composite Score for Conditions ---\n",
        "# Extract the same feature columns as in Code A\n",
        "feature_columns = [col for col in stationary_df.columns if col not in ['Y', 'Date']]\n",
        "column_to_index = {col: idx for idx, col in enumerate(feature_columns)}\n",
        "\n",
        "# Ensure your X_test_scaled is still accessible (before windowing)\n",
        "# Get the last `len(y_test_trimmed)` rows to match the trimmed labels\n",
        "X_test_base = X_test_scaled[-len(y_test_trimmed):]\n",
        "\n",
        "weights = {\n",
        "    'MXUS': 0.45, 'MXEU': 0.20, 'MXCN': 0.10,\n",
        "    'MXJP': 0.08, 'MXIN': 0.07, 'MXBR': 0.05, 'MXRU': 0.05\n",
        "}\n",
        "\n",
        "weighted_index_scaled = np.zeros(X_test_base.shape[0])\n",
        "for col, weight in weights.items():\n",
        "    if col in column_to_index:\n",
        "        weighted_index_scaled += X_test_base[:, column_to_index[col]] * weight\n",
        "\n",
        "vix_scaled = X_test_base[:, column_to_index['VIX']]\n",
        "composite_score = weighted_index_scaled - vix_scaled\n",
        "threshold_score = np.percentile(composite_score, 50)\n",
        "\n",
        "# Label each point based on market signal and anomaly\n",
        "market_condition = []\n",
        "for i, score in enumerate(composite_score):\n",
        "    if y_test_trimmed[i] == 1:\n",
        "        market_condition.append('Anomaly Up' if score > threshold_score else 'Anomaly Down')\n",
        "    else:\n",
        "        market_condition.append('Normal')\n",
        "\n",
        "# --- PCA Projection for Visualization ---\n",
        "pca = PCA(n_components=2)\n",
        "X_test_pca = pca.fit_transform(X_test_base)\n",
        "\n",
        "# --- Visual Style and Plotting ---\n",
        "plt.figure(figsize=(14, 10))\n",
        "size_map = {\n",
        "    'True Positive': 60,\n",
        "    'False Positive': 100,\n",
        "    'False Negative': 100,\n",
        "    'True Negative': 60\n",
        "}\n",
        "\n",
        "color_map = {\n",
        "    ('True Positive', 'Anomaly Up'): 'darkgreen',\n",
        "    ('True Positive', 'Anomaly Down'): 'salmon',\n",
        "    ('True Positive', 'Normal'): 'lime',\n",
        "    ('False Positive', 'Anomaly Up'): 'orange',\n",
        "    ('False Positive', 'Anomaly Down'): 'red',\n",
        "    ('False Positive', 'Normal'): 'gold',\n",
        "    ('False Negative', 'Anomaly Up'): 'teal',\n",
        "    ('False Negative', 'Anomaly Down'): 'purple',\n",
        "    ('False Negative', 'Normal'): 'pink',\n",
        "    ('True Negative', 'Anomaly Up'): 'gray',\n",
        "    ('True Negative', 'Anomaly Down'): 'gray',\n",
        "    ('True Negative', 'Normal'): 'gray'\n",
        "}\n",
        "\n",
        "def plot_group(mask, base_label, condition):\n",
        "    count = np.sum(mask)\n",
        "    if count > 0:\n",
        "        label = f\"{base_label} ({condition}) ({count})\"\n",
        "        plt.scatter(X_test_pca[mask, 0],\n",
        "                    X_test_pca[mask, 1],\n",
        "                    label=label,\n",
        "                    c=color_map[(base_label, condition)],\n",
        "                    s=size_map[base_label],\n",
        "                    alpha=0.7,\n",
        "                    edgecolors='k')\n",
        "\n",
        "# Build masks and plot\n",
        "y_true = y_test_trimmed\n",
        "y_pred_arr = y_pred\n",
        "market_condition_arr = np.array(market_condition)\n",
        "\n",
        "for actual, predicted, label in [(0, 0, 'True Negative'),\n",
        "                                  (0, 1, 'False Positive'),\n",
        "                                  (1, 0, 'False Negative'),\n",
        "                                  (1, 1, 'True Positive')]:\n",
        "    for condition in ['Anomaly Up', 'Anomaly Down', 'Normal']:\n",
        "        mask = (y_true == actual) & (y_pred_arr == predicted) & (market_condition_arr == condition)\n",
        "        plot_group(mask, label, condition)\n",
        "\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"LSTM Autoencoder Predictions with Market Anomaly Direction (PCA)\")\n",
        "plt.legend(fontsize=10, loc='best')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDs84DTwwQvX"
      },
      "source": [
        "## LSTM Autoencoder: True/False Positives/Negatives with Market Condition (3D UMAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PfVa5cgELUwy",
        "outputId": "2d57f0da-e978-46ec-b579-a01d7d0a56a0"
      },
      "outputs": [],
      "source": [
        "# --- Feature selection and composite score ---\n",
        "feature_columns = [col for col in stationary_df.columns if col not in ['Y', 'Date']]\n",
        "column_to_index = {col: idx for idx, col in enumerate(feature_columns)}\n",
        "\n",
        "X_test_base = X_test_scaled[-len(y_test_trimmed):]\n",
        "\n",
        "weights = {\n",
        "    'MXUS': 0.45, 'MXEU': 0.20, 'MXCN': 0.10,\n",
        "    'MXJP': 0.08, 'MXIN': 0.07, 'MXBR': 0.05, 'MXRU': 0.05\n",
        "}\n",
        "weighted_index_scaled = np.zeros(X_test_base.shape[0])\n",
        "for col, weight in weights.items():\n",
        "    if col in column_to_index:\n",
        "        weighted_index_scaled += X_test_base[:, column_to_index[col]] * weight\n",
        "\n",
        "vix_scaled = X_test_base[:, column_to_index['VIX']]\n",
        "composite_score = weighted_index_scaled - vix_scaled\n",
        "threshold_score = np.percentile(composite_score, 50)\n",
        "\n",
        "market_condition = []\n",
        "for i, score in enumerate(composite_score):\n",
        "    if y_test_trimmed[i] == 1:\n",
        "        market_condition.append('Anomaly Up' if score > threshold_score else 'Anomaly Down')\n",
        "    else:\n",
        "        market_condition.append('Normal')\n",
        "\n",
        "market_condition_arr = np.array(market_condition)\n",
        "\n",
        "# --- UMAP Projection ---\n",
        "umap_reducer = umap.UMAP(n_components=3, random_state=42)\n",
        "X_test_umap = umap_reducer.fit_transform(X_test_base)\n",
        "\n",
        "# --- Define true/false positive/negative labels ---\n",
        "y_true = y_test_trimmed\n",
        "y_pred_arr = y_pred\n",
        "\n",
        "# Label: e.g., 'True Positive (Anomaly Up)'\n",
        "points_label = []\n",
        "for i in range(len(y_true)):\n",
        "    if y_true[i] == 1 and y_pred_arr[i] == 1:\n",
        "        base = 'True Positive'\n",
        "    elif y_true[i] == 0 and y_pred_arr[i] == 1:\n",
        "        base = 'False Positive'\n",
        "    elif y_true[i] == 1 and y_pred_arr[i] == 0:\n",
        "        base = 'False Negative'\n",
        "    else:\n",
        "        base = 'True Negative'\n",
        "    points_label.append((base, market_condition[i]))\n",
        "\n",
        "points_label = np.array(points_label)\n",
        "\n",
        "# --- Color and size maps ---\n",
        "size_map = {\n",
        "    'True Positive': 60,\n",
        "    'False Positive': 100,\n",
        "    'False Negative': 100,\n",
        "    'True Negative': 60\n",
        "}\n",
        "color_map = {\n",
        "    ('True Positive', 'Anomaly Up'): 'darkgreen',\n",
        "    ('True Positive', 'Anomaly Down'): 'salmon',\n",
        "    ('True Positive', 'Normal'): 'lime',\n",
        "    ('False Positive', 'Anomaly Up'): 'orange',\n",
        "    ('False Positive', 'Anomaly Down'): 'red',\n",
        "    ('False Positive', 'Normal'): 'gold',\n",
        "    ('False Negative', 'Anomaly Up'): 'teal',\n",
        "    ('False Negative', 'Anomaly Down'): 'purple',\n",
        "    ('False Negative', 'Normal'): 'pink',\n",
        "    ('True Negative', 'Anomaly Up'): 'gray',\n",
        "    ('True Negative', 'Anomaly Down'): 'gray',\n",
        "    ('True Negative', 'Normal'): 'gray'\n",
        "}\n",
        "\n",
        "# --- 3D Plot ---\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "unique_labels = np.unique(points_label, axis=0)\n",
        "for base, condition in unique_labels:\n",
        "    mask = (points_label[:, 0] == base) & (points_label[:, 1] == condition)\n",
        "    if np.sum(mask) > 0:\n",
        "        label = f\"{base} ({condition}) ({np.sum(mask)})\"\n",
        "        ax.scatter(X_test_umap[mask, 0],\n",
        "                   X_test_umap[mask, 1],\n",
        "                   X_test_umap[mask, 2],\n",
        "                   c=color_map[(base, condition)],\n",
        "                   s=size_map[base],\n",
        "                   alpha=0.7,\n",
        "                   edgecolors='k',\n",
        "                   label=label)\n",
        "\n",
        "ax.set_xlabel(\"UMAP 1\")\n",
        "ax.set_ylabel(\"UMAP 2\")\n",
        "ax.set_zlabel(\"UMAP 3\")\n",
        "ax.set_title(\"LSTM Autoencoder: True/False Positives/Negatives with Market Condition (3D UMAP)\")\n",
        "ax.legend(fontsize=9, loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-LrJ2PUlI-2"
      },
      "source": [
        "## Local explainability: LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vj9x3rVCSQJw",
        "outputId": "38a0a135-7cd4-4729-9143-2943f92b1f23"
      },
      "outputs": [],
      "source": [
        "# --- 1) Genera sliding windows e flattening per LIME ---\n",
        "window_size = 20\n",
        "# X_train_windows, X_test_windows già creati come array (n_windows, window_size, n_features)\n",
        "# Li flatteniamo in (n_windows, window_size * n_features)\n",
        "X_train_flat = X_train_windows.reshape(X_train_windows.shape[0], -1)\n",
        "X_test_flat  = X_test_windows.reshape(X_test_windows.shape[0], -1)\n",
        "\n",
        "# Feature names: \"t{t}_{feat_name}\"\n",
        "base_feature_names = stationary_df.columns.tolist()  # i 42 indicatori\n",
        "feature_names = [\n",
        "    f\"t{t}_{fname}\"\n",
        "    for t in range(window_size)\n",
        "    for fname in base_feature_names\n",
        "]\n",
        "\n",
        "class_names = ['Normal', 'Anomaly']\n",
        "\n",
        "# --- 2) Inizializza LIME sull’input flattenato ---\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data         = X_train_flat,\n",
        "    feature_names         = feature_names,\n",
        "    class_names           = class_names,\n",
        "    mode                  = 'classification',\n",
        "    discretize_continuous = True,\n",
        "    random_state          = seed\n",
        ")\n",
        "\n",
        "# --- 3) Definisci la funzione di predict per LIME ---\n",
        "#    usa il modello LSTM autoencoder e la soglia determinata in validazione\n",
        "def predict_fn(flat_data):\n",
        "    \"\"\"\n",
        "    flat_data: array di shape (n_instances, window_size * n_features)\n",
        "    Restituisce array (n_instances, 2) con probabilità [P(Normal), P(Anomaly)].\n",
        "    \"\"\"\n",
        "    # ricostruisci il formato (batch, window_size, n_features)\n",
        "    arr = flat_data.reshape(flat_data.shape[0], window_size, -1)\n",
        "    tensor = torch.tensor(arr).float().to(device)\n",
        "\n",
        "    # calcola errori di ricostruzione\n",
        "    with torch.no_grad():\n",
        "        x_recon, _ = model(tensor)\n",
        "        errors = ((x_recon - tensor)**2).mean(dim=(1,2)).cpu().numpy()\n",
        "\n",
        "    # classificazione binaria con soglia\n",
        "    preds = (errors > threshold).astype(int)\n",
        "\n",
        "    # probabilità deterministiche (1.0 sulla classe scelta)\n",
        "    probs = np.zeros((len(preds), 2))\n",
        "    probs[np.arange(len(preds)), preds] = 1.0\n",
        "    return probs\n",
        "\n",
        "# --- 4) Spiega un’istanza di test ---\n",
        "idx = 12  # scegli l’istanza\n",
        "exp = explainer.explain_instance(\n",
        "    data_row     = X_test_flat[idx],\n",
        "    predict_fn   = predict_fn,\n",
        "    num_features = 20       # mostra le top-20 feature\n",
        ")\n",
        "\n",
        "# Stampa contributi\n",
        "for feat, weight in exp.as_list():\n",
        "    print(f\"{feat:30s} -> {weight:+.4f}\")\n",
        "\n",
        "# Verifica etichetta e ricostruzione\n",
        "print(f\"\\nTrue label:       {y_test_trimmed[idx]}\")\n",
        "print(f\"Ricostruzione err: {compute_reconstruction_error(model, X_test_tensor[idx:idx+1])[0]:.6f}\")\n",
        "print(f\"Soglia:           {threshold:.6f}\")\n",
        "\n",
        "# (Opzionale) visualizza il grafico\n",
        "fig = exp.as_pyplot_figure()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGYyv70HdUP_",
        "outputId": "b01aa0ba-4d2d-4b3f-bb73-77d014e92fed"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- MODELS RESULTS VISUALIZATION ---\\n\")\n",
        "# Plot the metrics for all models\n",
        "print(metrics_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdt9_EnLdR8U"
      },
      "source": [
        "# Global explainability: zeroing out each feature and computing increase in error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "id": "M_IxX6p6dpRC",
        "outputId": "b0ad1cf0-35a1-4f47-d4fd-9232b9f59d20"
      },
      "outputs": [],
      "source": [
        "def analyze_feature_importance_lstm(model, X_tensor, feature_names):\n",
        "    \"\"\"\n",
        "    Analyze feature importance by measuring impact of each feature on reconstruction error (LSTM Autoencoder)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    n_features = X_tensor.shape[2]\n",
        "    importance_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        original_recon, _ = model(X_tensor)\n",
        "        original_error = ((original_recon - X_tensor) ** 2).mean(dim=(1, 2)).cpu().numpy()\n",
        "\n",
        "    for i in range(n_features):\n",
        "        perturbed_input = X_tensor.clone()\n",
        "        perturbed_input[:, :, i] = 0.0  # Zero out feature i across all time steps\n",
        "\n",
        "        with torch.no_grad():\n",
        "            perturbed_recon, _ = model(perturbed_input)\n",
        "            perturbed_error = ((perturbed_recon - X_tensor) ** 2).mean(dim=(1, 2)).cpu().numpy()\n",
        "\n",
        "        # Importance = mean increase in reconstruction error\n",
        "        importance = np.mean(perturbed_error - original_error)\n",
        "        importance_scores.append(importance)\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importance_scores\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20), palette='mako')\n",
        "    plt.title('Top 20 Features by Importance (LSTM Autoencoder)', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "feature_names = stationary_df.columns\n",
        "importance_df = analyze_feature_importance_lstm(model, X_test_tensor, feature_names)\n",
        "print(importance_df.head(10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d426937379742b6881cc20f50cb4c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30c610c2fcc442e7a3d8f3d5e8ca193f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a161be26ba44f2d996da4e830bf37f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30c610c2fcc442e7a3d8f3d5e8ca193f",
            "placeholder": "​",
            "style": "IPY_MODEL_f5a8b5f127e3404ba43f03570a0388dd",
            "value": " 50/50 [32:24&lt;00:00, 38.91s/it]"
          }
        },
        "5b89f25654724d4386f1404a6e5f633a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_816a0aff13ae43288bfe43885fcdc094",
            "placeholder": "​",
            "style": "IPY_MODEL_1d426937379742b6881cc20f50cb4c84",
            "value": "Best trial: 15. Best value: 0.573249: 100%"
          }
        },
        "6492102a614947c7ac50ff871d7b3bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa2d2a19f693498b94a1e5ca703307d9",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faabdf581a2c49e0b1b7acc555d60413",
            "value": 50
          }
        },
        "816a0aff13ae43288bfe43885fcdc094": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78f488724cc49de897dfd3b3193944f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b89f25654724d4386f1404a6e5f633a",
              "IPY_MODEL_6492102a614947c7ac50ff871d7b3bb5",
              "IPY_MODEL_5a161be26ba44f2d996da4e830bf37f9"
            ],
            "layout": "IPY_MODEL_aa10551466834e8b8a1904484f80a36a"
          }
        },
        "aa10551466834e8b8a1904484f80a36a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa2d2a19f693498b94a1e5ca703307d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a8b5f127e3404ba43f03570a0388dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faabdf581a2c49e0b1b7acc555d60413": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
